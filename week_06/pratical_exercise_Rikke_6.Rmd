---
title: "practical_exercise_6, Methods 3, 2021, autumn semester"
author: '[FILL IN YOUR NAME]'
date: "[FILL IN THE DATE]"
output: html_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>


# Exercises and objectives

1) Get acquainted with _Python_, and learn some of the differences between it and _R_  
2) Estimate bias and variance based on a true underlying function  

REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below (__MAKE A KNITTED VERSION__)  
REMEMBER: All exercises should be done in _Python_

# EXERCISE 1 Get acquainted with _Python_ learn some of the differences between it and _R_  

To make sure that _Python_ runs within _R Markdown_, make sure you have the _reticulate_ package installed `install.packages('reticulate')`  
Also create a text file that is called _.Renviron_ (remember the dot) placed in the folder where your _RProj_ file is. It should have a single line: `RETICULATE_PYTHON=PATH` where `PATH` is the path to your _methods3_ conda environment. Use the commands below to find the paths:  

```{r}
install.packages('reticulate')
install.packages('Rcpp')
library(reticulate)

pacman::p_load(reticulate, Rcpp)
print(conda_list())
```

To update your environment based on the updated `methods3_environment.yml` file, go to your _week_06_ folder and run the following from a _bash_ interpreter (e.g. _terminal_):  

```{bash, eval=FALSE}
conda env create --force -f methods3_environment.yml
```

The `--force` flag allows for overwriting  

## Good to know about _Python_ (in no particular order)

```{python}
## assignment is done and only done with "=" (no arrows)
a = 2
# a <- 2 # results in a syntax error
## already assigned variables can be reassigned with basic arithmetic operations
a += 2 #her adder vi 2 til a until furhter notice
print(a)
a -= 1
print(a)
a *= 4
print(a)
a //= 2 # integer division
print(a)
a /= 2 # float  (numeric from R) division
print(a)
a **= 3 # exponentiation 
print(a)

a_list = [1, 2] # initiate a list (the square brackets) with the integers 1 and 2
b = a_list ## b now points to a_list, not to a new list with the integers 1 and 2 

a_list.append(3) # add a new value to the end of the list
print(a_list)
print(b) # make sure you understand this

print(a_list[0]) # zero-indexing
print(a_list[1])


new_list = [0, 1, 2, 3, 4, 5]
print(new_list[0:3])  # slicing 
#you can also do it backwards by using -1
print(new_list[0:-1])

for index in range(0, 5): # indentation (use tabulation) controls scope of control variables 
    #(no brackets necessary),
    if index == 0: # remember the colon
        value = 0
    else:
        value += index
    print(value)
# det er meget vigtigt at du bruger "tab" for at komme ind på den rigtige linje (så alt i for loop er i samme niveau)

  
this_is_true = True # logical values
this_is_false = False
    
# define functions using def
def fix_my_p_value(is_it_supposed_to_be_significant):
    if is_it_supposed_to_be_significant:
        p = 0.01
    else:
        p = 0.35
    return(p)

print(fix_my_p_value(True))


import numpy # methods of numpy can now be accessed as below, her er "numpy" en pakke
# importing packages (similar to library)
# så hver gang du bruger en funktion fra numpy, skal du skrive numpy.arrange() !!!!!!!!!!!!!

print(numpy.arange(1, 10)) # see the dot
print(numpy.abs(-3))
import numpy as np # you can import them with another name than its default
#this way every function is now called only e.g.  np.arrange()!!!!!!!!!!!!!
print(np.cos(np.pi))
from numpy import pi, arange # or you can import specific methods
print(arange(1, 7))
print(pi)


matrix = np.ones(shape=(5, 5)) # create a matrix of ones
identity = np.identity(5) # create an identity matrix (5x5)
identity[:, 2] = 5 # exchange everything in the second column with 5's
# colon means "everything" here :))

## no dots in names - dots indicate applying a method like the dollar sign $ in R

import matplotlib.pyplot as plt
plt.figure() # create new figure
plt.plot([1, 2], [1, 2], 'b-') # plot a blue line
# plt.show() # show figure
plt.plot([2, 1], [2, 1], 'ro') # scatter plot (red)
# plt.show()
plt.xlabel('a label')
plt.title('a title')
plt.legend(['a legend', 'another legend'])
plt.show()
```

1) Do a linear regression based on _x_, _X_ and _y_ below (_y_ as the dependent variable) (Exercise 1.1)  
    i. find $\hat{\beta}$ and $\hat{y}$ (@ is matrix multiplication)
    ii. plot a scatter plot of _x_, _y_ and add a line based on $\hat{y}$ (use `plt.plot` after running `import matplotlib.pyplot as plt`)  
2) Create a model matrix, $X$ that estimates, $\hat\beta$ the means of the three sets of observations below, $y_1, y_2, y_3$ (Exercise 1.2)
    i. find $\hat\beta$ based on this $X$  
    ii. Then create an $X$ where the resulting $\hat\beta$ indicates: 1) the difference between the mean of $y_1$ and the mean of $y_2$; 2) the mean of $y_2$; 3) the difference between the mean of $y_3$ and the mean of $y_1$  
    
    ^ ANOVAAAAAA ^
    
3) Finally, find the F-value for this model (from exercise 1.2.ii) and its degrees of freedom. What is the _p_-value associated with it? (You can import the inverse of the cumulative probability density function `ppf` for _F_ using `from scipy.stats import f` and then run `1 - f.ppf`)
    i. plot the probability density function `f.pdf` for the correct F-distribution and highlight the _F_-value that you found  
    ii. how great a percentage of the area of the curve is to right of the highlighted point

```{python}
import pandas as pd
from matplotlib import pyplot as plt

# Exercise 1.1
import numpy as np
np.random.seed(7) # for reproducibility

x = np.arange(10)
y = 2 * x# alle værdier i x ganget med 2 [ 0  2  4  6  8 10 12 14 16 18]
y = y.astype(float) #lav til decimal tal
n_samples = len(y) # 10 værdier
y += np.random.normal(loc=0, scale=1, size=n_samples) #loc = mean (centre of distribution), scale = sd of 1, size= antal af values 

X = np.zeros(shape=(n_samples, 2))# matrix med empty entries 
print(X)

#putting values in designmatrix
X[:, 0] = x ** 0 # alt opløftet i 0 er 1 (første række (0) i vores designmatrix)
X[:, 1] = x ** 1 # x opløftet i 1 (anden række (1) i vores designmatrix) 

#our X, x and y values
print(X)
print(x)
print(y)


# BETAHAT transpose ud fra det her solve(t(X) %*% X) %*% t(X) %*% y
bhat = np.dot(np.linalg.inv(np.dot(X.T,X)),np.dot(X.T,y))


print(bhat)


# YHAT  ud fra det her yhat <- X %*% bhat
yhat = np.matmul(X, bhat) #matrix multiplication
print(yhat)


# MY TAKE
import numpy as np 
import matplotlib.pyplot as plt

plt.plot(x, y, 'o')#create basic scatterplot
plt.plot(x, bhat[1]*x+bhat[0])#add linear regression line to scatterplot 
plt.plot(x, yhat, 'o', color='red')#use green as color for individual points
plt.xlabel('x')
plt.title('MY first linear regression plot wehuuuu')
plt.legend(['regression line', 'predicted values'])
plt.show()

```


2) Create a model matrix, $X$ that estimates, $\hat\beta$ the means of the three sets of observations below, $y_1, y_2, y_3$ (Exercise 1.2)
    i. find $\hat\beta$ based on this $X$  
    ii. Then create an $X$ where the resulting $\hat\beta$ indicates: 1) the difference between the mean of $y_1$ and the mean of $y_2$; 2) the mean of $y_2$; 3) the difference between the mean of $y_3$ and the mean of $y_1$  
    
    ^ ANOVAAAAAA ^

```{python}
# Exercise 1.2
y1 = np.array([3, 2, 7, 6, 9])
y2 = np.array([10, 4, 2, 1, -3])
y3 = np.array([15, -2, 0, 0, 3])
y = np.concatenate((y1, y2, y3))


# Exercise 1.1
import numpy as np
np.random.seed(7) # for reproducibility

x = np.arange(10)
y = 2 * x
y = y.astype(float)
n_samples = len(y)
y += np.random.normal(loc=0, scale=1, size=n_samples)

X = np.zeros(shape=(n_samples, 2))
X[:, 0] = x ** 0 # alt opløftet i 0 er 1 (første række i vores designmatrix)
X[:, 1] = x ** 1 # x opløftet i 1 (anden række i vores designmatrix) 


```

# EXERCISE 2 - Estimate bias and variance based on a true underlying function  

*Exercise for you: In tomorrow’s exercise: For each question indicate whether you understood what was required of you.* 

We can express regression as $y = f(x) + \epsilon$ with $E[\epsilon] = 0$ and $var(\epsilon) = \sigma^2$ ($E$ means expected value)  
  
For a given point: $x_0$, we can decompose the expected prediction error , $E[(y_0 - \hat{f}(x_0))^2]$ into three parts - __bias__, __variance__ and __irreducible error__ (the first two together are the __reducible error__):

The expected prediction error is, which we also call the __Mean Squared Error__:  
$E[(y_0 - \hat{f}(x_0))^2] =  bias(\hat{f}(x_0))^2 + var(\hat{f}(x_0)) + \sigma^2$
  
where __bias__ is;
  
$bias(\hat{f}(x_0)) = E[\hat{f}(x_0)] - f(x_0)$

1) Create a function, $f(x)$ that squares its input. This is our __true__ function  
    i. generate data, $y_{true}$, based on an input range of [0, 6] with a spacing of 0.1. Call this $x$
    ii. add normally distributed noise to $y_{true}$ with $\sigma=5$ (set a seed to 7 `np.random.seed(7)`) and call it $y_{noise}$
    iii. plot the true function and the generated points  
    
```{python}
np.random.seed(7)
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt

#create a function that squares its input
def true_function(x):
    return x**2


# i. generate data, ytrue, based on an input range of [0, 6] with a spacing of 0.1 
np.arrange()
x <- seq(0,6,0.1)


# ii. add normally distributed noise to ytrue with σ = 5 (set a seed to 7 np.random.seed(7)) and call it y_noise
y_true <- rnorm(10, sd=5)

# iii. plot the true function and the generated points


```


2) Fit a linear regression using `LinearRegression` from `sklearn.linear_model` based on $y_{noise}$ and $x$ (see code below). 
    i. plot the fitted line (see the `.intercept_` and `.coef_` attributes of the `regressor` object) on top of the plot (from 2.1.iii)
    ii. now run the code associated with Exercise 2.2.ii - what does X_quadratic amount to?
    iii. do a quadratic and a fifth order fit as well and plot them (on top of the plot from 2.2.i)
    
```{r}
# i. plot the fitted line (see the `.intercept_` and `.coef_` attributes of the `regressor` object) on top of the plot 



# ii. now run the code associated with Exercise 2.2.ii - what does X_quadratic amount to?

# iii. do a quadratic and a fifth order fit as well and plot them (on top of the plot from 2.2.i)

```

3) Simulate 100 samples, each with sample size `len(x)` with $\sigma=5$ normally distributed noise added on top of the true function  
    i. do linear, quadratic and fifth-order fits for each of the 100 samples  
    ii create a __new__ figure, `plt.figure`, and plot the linear and the quadratic fits (colour them appropriately); highlight the true value for $x_0=3$. From the graphics alone, judge which fit has the highest bias and which has the highest variance  
    iii. create a __new__ figure, `plt.figure`, and plot the quadratic and the fifth-order fits (colour them appropriately); highlight the true value for $x_0=3$. From the graphics alone, judge which fit has the highest bias and which has the highest variance  
    iv. estimate the __bias__ and __variance__ at $x_0$ for the linear, the quadratic and the fifth-order fits (the expected value $E[\hat{f}(x_0)]$ is found by taking the mean of all the simulated, $\hat{f}(x_0)$, differences)  
    v. show how the __squared bias__ and the __variance__ are related to the complexity of the fitted models  
    vi. simulate __epsilon__: `epsilon = np.random.normal(scale=5, size=100)`. Based on your simulated values of __bias, variance and epsilon__, what is the __Mean Squared Error__ for each of the three fits? Which fit is better according to this measure?  


```{python, eval=FALSE}
# Exercise 2.2
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit() ## what goes in here?
```

```{python, eval=FALSE}
# Exercise 2.2.ii
from sklearn.preprocessing import PolynomialFeatures
quadratic = PolynomialFeatures(degree=2)
X_quadratic = quadratic.fit_transform(x.reshape(-1, 1))
regressor = LinearRegression()
regressor.fit() # what goes in here?


```



# Python book exercise - chapter 10

### Housing dataset: 
The features of the 506 samples in the Housing dataset are summarized here, taken from the original source that was previously shared on https://archive.ics.uci. edu/ml/datasets/Housing:
• CRIM: Per capita crime rate by town
• ZN: Proportion of residential land zoned for lots over 25,000 sq. ft.
• INDUS: Proportion of non-retail business acres per town
• CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
• NOX: Nitric oxide concentration (parts per 10 million)
• RM: Average number of rooms per dwelling
• AGE: Proportion of owner-occupied units built prior to 1940
• DIS: Weighted distances to five Boston employment centers
• RAD: Index of accessibility to radial highways
• TAX: Full-value property tax rate per $10,000
• PTRATIO: Pupil-teacher ratio by town
• B: 1000(Bk - 0.63)^2, where Bk is the proportion of [people of African American descent] by town
• LSTAT: Percentage of lower status of the population
• MEDV: Median value of owner-occupied homes in $1000s


```{python}
#set up package
import pandas as pd

#load data from github txt file
df = pd.read_csv('https://raw.githubusercontent.com/rasbt/python-machine-learning-book-2nd-edition/master/code/ch10/housing.data.txt',header=None, sep='\s+')

#bogen vil gerne have at vi loader ind sådan her, men det kan jeg ikke få til at fungere
#jeg har gemt txt filen ned i week6 mappen, nvm
dff = pd.read_csv('./housing.data.txt'), sep='\s+')

#define columns
df.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD','TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']

#view first data
df.head()


#get plotting (SCATTERPLOT MATRIX)
import matplotlib.pyplot as plt
import seaborn as sns
cols = ['LSTAT', 'INDUS', 'NOX', 'RM', 'MEDV']
sns.pairplot(df[cols], size=0.9)
plt.tight_layout()
plt.show()


#plot of correlation matrix #jeg kan fucking ik få det til at virke
import numpy as np
cm = np.corrcoef(df[cols].values.T)
sns.set(font_scale=1.5)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size':15},yticklabels=cols, xticklabels=cols)
plt.show()
#irl ser det sejt ud






```

#### Fitting at linear regression 

#To fit a linear regression model, we are interested in those features that have a high correlation with our target variable MEDV (Median value of owner-occupied homes in $1000s).

Looking at the previous correlation matrix, we see that our target variable MEDV shows the largest correlation with the LSTAT variable (-0.74); however, as you might remember from inspecting the scatterplot matrix, there is a clear nonlinear relationship between LSTAT and MEDV. On the other hand, the correlation between RM and MEDV is also relatively high (0.70). Given the linear relationship between these two variables that we observed in the scatterplot,*RM seems to be a good choice for an exploratory variable to introduce the concepts of a simple linear regression model in the following section*.

```{r}

```




