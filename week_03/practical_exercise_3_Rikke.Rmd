---
title: "practical_exercise_3, Methods 3, 2021, autumn semester"
author: 'Rikke Uldb√¶k'
date: "29/9 2021"
output: html_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/Cognitive Science/3rd semester/Methods 3/github_methods_3/week_03")
pacman::p_load(tidyverse,dplyr, data.table, vroom, ggplot2, readbulk, lme4, rstanarm, MuMIn)
```

# Exercises and objectives
The objectives of the exercises of this assignment are:  
1) Download and organise the data and model and plot staircase responses based on fits of logistic functions  
2) Fit multilevel models for response times  
3) Fit multilevel models for count data  

REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below (__MAKE A KNITTED VERSION__)  
REMEMBER: This assignment will be part of your final portfolio

## Exercise 1

Go to https://osf.io/ecxsj/files/ and download the files associated with Experiment 2 (there should be 29).  
The data is associated with Experiment 2 of the article at the following DOI https://doi.org/10.1016/j.concog.2019.03.007  

1) Put the data from all subjects into a single data frame  
```{r}
#read the data
data <- read_bulk(
  directory= "experiment_2/",
  fun = read_csv
)

attach(data)
```

2) Describe the data and construct extra variables from the existing variables  
    i. add a variable to the data frame and call it _correct_ (have it be a _logical_ variable). Assign a 1 to each row where the subject indicated the correct answer and a 0 to each row where the subject indicated the incorrect answer (__Hint:__ the variable _obj.resp_ indicates whether the subject answered "even", _e_ or "odd", _o_, and the variable _target_type_ indicates what was actually presented.
    ii. describe what the following variables in the data frame contain, _trial.type_, _pas_, _trial_, _target.contrast_, _cue_, _task_, _target_type_, _rt.subj_, _rt.obj_, _obj.resp_, _subject_ and _correct_. (That means you can ignore the rest of the variables in your description). For each of them, indicate and argue for what `class` they should be classified into, e.g. _factor_, _numeric_ etc.  
    


### i-ii. Describe the data  

For each of them, indicate and argue for what `class` they should be classified into, e.g. _factor_, _numeric_ etc.  

##### trial.type
This variable contains two different values; "experiment" and "staircase" in which each represent two different trials. When eyeballing this variable, it is evident what we have 69% "experiment" values and 31% "staircase" variables. This variable is currently a character variable and will be classified into a factor for analytic purposes.

```{r}
class(trial.type)
data$trial.type <- as.factor(data$trial.type)

#plot
n <- nrow(data)  # Number of rows in total
(percent_trial.type<- table(data$trial.type)/n * 100) #generating percentages of trial.type
barplot(percent_trial.type,ylim=c(0,100), ylab="percent",main="Barplot of trial.type")
```

##### pas
The "pas" variable (an abbreviation of Perceptual Awareness Scale), contain 4 different values ranging from 1:4. 1 indicates No Experience, 2 indicates Weak Glimpse (WG), 3 indicates Almost Clear Experience (ACE), and 4 indicates Clear Experience (CE). A histogram of the frequency, shows that the most frequent value is 1 and the least frequent value is 3. This variable is currently a numeric variable and will be classified into a factor for analytic purposes.


```{r}
class(pas)
summary(pas)
data$pas <- as.factor(data$pas)
hist(pas)
```

##### trial
The "trial" variable contains values ranging from 1:73. This variables indicate that there are 73 different trials in the data set. This variable is currently a numeric variable and will be classified into a factor for analytic purposes.


```{r}
class(trial)
data$trial <- as.factor(data$trial)
```

##### target.contrast
The "target.contrast" variable is a numeric variable ranging from 0-1, representing the grey-scale proportion of the target digit.
```{r}
class(target.contrast)
ggplot(data, aes(target.contrast))+geom_density()
summary(target.contrast)

```

##### cue
The "cue" variable is a numeric variable, representing a number code for cue.

```{r}
class(cue)
summary(cue)
hist(cue)
```

##### task
The "task" variable consists of 3 different values: "singles", "pairs" and "quadruplets". These 3 different values each represent the number of digits that would be presented to the participant. An example of "singles" being 2:3, an example of "pairs" being 12:44, and an example of quadruplets being 5432:2345.This variable is currently a character variable and will be classified into a factor for analytic purposes.

```{r}
class(task)
data$task <- as.factor(data$task)

#plot
n <- nrow(data)  # Number of rows in total
(percent_task<- table(data$task)/n * 100) #generating percentages of trial.type
barplot(percent_task,ylim=c(0,100), ylab="percent",main="Barplot of task")
```

##### target.type
The "target.type" variable represents if the digit (target) was either an even or odd number. 
This variable is currently a character variable and will be classified into a factor for analytic purposes.
```{r}
class(target.type)
data$target.type <- as.factor(data$target.type)
```

##### rt.subj
The "rt.subj" variable represents the reaction time (seconds) on the PAS response. The variable is numeric and highly right skewed, wwith a minimum value of 0.01369, a maximum value of 32.77161, and a mean of 0.74128.  
```{r}
class(rt.subj)
ggplot(data, aes(rt.subj))+ geom_density()
summary(rt.subj)
```


##### rt.obj
The variable "rt.obj" represents the reaction time (seconds) on the target digit. This variable is numeric, and highly right skewed, with a minimum value of 0.00004, a maximum value of 291.83119 , and a mean of 1.16186.  
```{r}
class(rt.obj)
ggplot(data, aes(rt.obj))+ geom_density()
summary(rt.obj)
```


##### obj.resp
The character variable "obj.resp" represents the key actually pressed e for even and o for odd.This variable is currently a character variable and will be classified into a factor for analytic purposes.
```{r}
class(obj.resp)
data$cue <- as.factor(data$obj.resp) 
```

##### subject
The subject variable represents each participant's individual ID, which makes it easier for us to distinguish between the participants and account for possible individual differences. This variable is a character string containing 29 different subject ID's, and each subject ID has 506 rows (observations). This variable is currently a character variable and will be classified into a factor for analytic purposes.

```{r}
class(subject)
length(unique(subject)) #amount of different participants
data$subject <- as.factor(data$subject)
```

##### correct
###### Add "correct" variable
The "correct" variable is a logical binary variable containing 1's and 0's. 1 demonstrates when the subject indicated a correct answer (i.e. target_type = "even" and obj.resp = "e", and target_type = "odd" and obj.resp = "o"), and 0 demonstrates when the subject indicated an incorrect answer. 

```{r}
#add a variable and call it "correct"
data$correct <- (ifelse(obj.resp== "o" & target.type == "odd","1",
                               ifelse(obj.resp=="e" & target.type== "even","1", 0)))
#making it as a factor
data$correct <- as.factor(data$correct)
```
 

    iii. for the staircasing part __only__, create a plot for each subject where you plot the estimated function (on the _target.contrast_ range from 0-1) based on the fitted values of a model (use `glm`) that models _correct_ as dependent on _target.contrast_. These plots will be our _no-pooling_ model. Comment on the fits - do we have enough data to plot the logistic functions? 

Lau, I don't really get this exercise. I'm not sure you mean either complete pooling or no pooling, so I did both. 


### iii.  Complete pooling/ No pooling (?)
The fits looks quite poor, because the fitted values does not resemble the sigmoid function.

```{r}
#iii. 
#----complete pooling model------#
# Creating df only containing staircase trials
staircase <- data %>% 
  subset(trial.type == "staircase")

# Complete pooling model
complete_pool_model <- glm(correct ~ target.contrast, data = staircase, family=binomial)

# Complete pooling plot
ggplot(staircase, (aes(x = target.contrast, y = correct, color= subject)))+
  geom_point()+
  geom_line(aes(target.contrast, fitted(complete_pool_model))) +
  facet_wrap(.~subject)+ 
  labs(title = "Complete Pooling") +
  theme_bw()

#----no pooling model------#
# No pooling with interaction effect with subject
no_pooling_model<- glm(correct ~ target.contrast+ target.contrast*subject, data = staircase, family="binomial")

# No pooling with interaction effect with subject PLOT
ggplot(staircase, (aes(x = target.contrast, y = correct, color=subject)))+ 
  geom_point()+
  geom_line(aes(target.contrast,fitted(no_pooling_model) ), color = "black") +
  facet_wrap(.~subject)+ 
  labs(title = "No Pooling")


## another take, no pooling, glm for each subject 
dfStair = data %>% 
  subset(trial.type == "staircase")

dfStair$subject = gsub("(?<![0-9])0+", "", dfStair$subject, perl = TRUE)
dfStair$subject = as.integer(dfStair$subject)

nopoolfun <- function(i){
  dat <- dfStair[which(dfStair$subject == i),]
  model <- glm(correct~target.contrast, family = 'binomial', data=dat)
  fitted <- model$fitted.values
  plot_dat <- data.frame(cbind(fitted,'target.contrast'=dat$target.contrast))
plot <- ggplot(plot_dat, aes(x = target.contrast, y = fitted))+
  geom_point()+
  geom_line(aes(x = target.contrast, y = fitted))+
  xlab('Target Contrast')+
  ylim(c(0,1))+
  theme_minimal()
print(plot)
}

# Running the function for each participant
for (i in 1:29){
  nopoolfun(i)
}

library(gridExtra)
subjects <- c(1:16)
plots <- lapply(subjects, FUN=nopoolfun)
do.call(grid.arrange,  plots)

subjects <- c(17:29)
plots <- lapply(subjects, FUN=nopoolfun)
do.call(grid.arrange,plots)

```

### iv. partial pooling
    iv. on top of those plots, add the estimated functions (on the _target.contrast_ range from 0-1) for each subject based on partial pooling model (use `glmer` from the package `lme4`) where unique intercepts and slopes for _target.contrast_ are modelled for each _subject_  


```{r}
# Partial pooling model
partial_pool_model <- glmer(correct~target.contrast + (target.contrast|subject), data = staircase, family = "binomial")

# plot
ggplot(staircase, (aes(x = target.contrast, y = correct, color=subject)))+ 
  geom_point()+
  geom_line(aes(target.contrast, fitted(partial_pool_model)), color = "black") +
  facet_wrap(.~subject)+ 
  labs(title = "Partial pooling")

```
 

### v - Describe in own words
v. in your own words, describe how the partial pooling model allows for a better fit for each subject. 


Compared to no pooling and complete pooling models, partial pooling provides better insight about the population (general tendencies), while allowing the model to take individual differences into account (individual baselines). With partial pooling we can both make a great fit for each subject and make it generalizable (in contrast to no pooling). 


## Exercise 2

Now we __only__ look at the _experiment_ trials (_trial.type_)  

1) Pick four subjects and plot their Quantile-Quantile (Q-Q) plots for the residuals of their objective response times (_rt.obj_) based on a model where only intercept is modelled  
    i. comment on these    
    ii. does a log-transformation of the response time data improve the Q-Q-plots?  

#### 1. i-ii. QQ-plot of residuals

Before log transformation of the 4 subjects objective response time based on the model with only an intercept, the residuals indicate a right skewed tail, as they deviate from the line, i.e deviate from normality. 

After log transformation of the 4 subjects objective response time based on the model with only an intercept, the residuals indicate both a left skewed tail and a tiny right skewed tail. The log transformation of the objective response time data (variable= "rt.obj") improves the Q-Q-plot, as the dots generally falls better on the line.

```{r}
#experiment dataframe
experiment <- data %>% 
  subset(data$trial.type == "experiment")

# df + model for each subject
subject1 <-  experiment %>% 
  filter(subject == "001") %>% 
  mutate("log_rt.obj" = log(rt.obj))

subjects_model1 <- lm(rt.obj~1, data= subject1)
subjects_model_log1 <- lm(log_rt.obj~1, data= subject1) #log trans

# df + model for each subject
subject2 <-  experiment %>% 
  filter(subject == "002") %>% 
  mutate("log_rt.obj" = log(rt.obj))

subjects_model2 <- lm(rt.obj~1, data= subject2)
subjects_model_log2 <- lm(log_rt.obj~1, data= subject2)#log trans

# df + model for each subject
subject3 <-  experiment %>% 
  filter(subject == "003") %>% 
  mutate("log_rt.obj" = log(rt.obj))

subjects_model3 <- lm(rt.obj~1, data= subject3)
subjects_model_log3 <- lm(log_rt.obj~1, data= subject3)#log trans

# df + model for each subject
subject4 <-  experiment %>% 
  filter(subject =="004") %>% 
  mutate("log_rt.obj" = log(rt.obj))

subjects_model4 <- lm(rt.obj~1, data= subject4)
subjects_model_log4 <- lm(log_rt.obj~1, data= subject4)#log trans


#plotting the residuals in a QQ-plot
qqnorm(resid(subjects_model1))
qqline(resid(subjects_model1))

qqnorm(resid(subjects_model2))
qqline(resid(subjects_model2))

qqnorm(resid(subjects_model3))
qqline(resid(subjects_model3))

qqnorm(resid(subjects_model4))
qqline(resid(subjects_model4))


#plotting the residuals after log-transformation
qqnorm(resid(subjects_model_log1))
qqline(resid(subjects_model_log1))

qqnorm(resid(subjects_model_log2))
qqline(resid(subjects_model_log2))

qqnorm(resid(subjects_model_log3))
qqline(resid(subjects_model_log3))

qqnorm(resid(subjects_model_log4))
qqline(resid(subjects_model_log4))
```


2) Now do a partial pooling model modelling objective response times as dependent on _task_? (set `REML=FALSE` in your `lmer`-specification)  
    i. which would you include among your random effects and why? (support your choices with relevant measures, taking into account variance explained and number of parameters going into the modelling)  
    ii. explain in your own words what your chosen models says about response times between the different tasks  

#### 2. i-ii.Partial pooling

i. 
The partial pooling model 4 (log(rt.obj) ~ task + (1 + task | subject) + (1 | trial)), has the lowest sigma as well as AIC value. This is the model that predicts objective response times (rt.obj) dependent on task (fixed effects), modelling a random intercepts for subjects and trial as well as random slopes for task. Random intercepts were modeled for subject as one would expect individual baseline performance differences among different participants. Trial as also been modelled as random intercepts as you would expect different trials to vary in difficulty.


ii. 
The estimates for quadruplet and singles are negative, which indicates that participants have a faster objective response time compared to the intercept (pair). When comparing the tasks "quadruplet" and "singles" the people are generally faster to the "single" task. The effect is, however, strongest from changing from *pairs* to *singles* (the respodents generally complete the singles tasks faster, relative to the quadruplet tasks).
  

```{r}
#partial pooling models
partial1 <- lmerTest::lmer(log(rt.obj)~task + (1|subject) + (1|trial), data = experiment, REML=FALSE)

partial2 <- lmerTest::lmer(log(rt.obj)~task + (1|trial), data = experiment, REML=FALSE)

partial3 <- lmerTest::lmer(log(rt.obj)~task + (1|subject), data = experiment, REML=FALSE)

partial4 <- lmerTest::lmer(log(rt.obj) ~ task + (1 + task | subject) + (1 | trial), data = experiment, REML = F)

partial5 <- lmerTest::lmer(log(rt.obj) ~ task + (1 + task | subject), data = experiment, REML=FALSE)

#Akaike information criterion
AIC(partial1, partial2, partial3, partial4, partial5)

#sigma
partial_sigma<- c(sigma(partial1),sigma(partial2),sigma(partial3), sigma(partial4), sigma(partial5));print(partial_sigma)

#pesudo R2
r.squaredGLMM(partial1) # R2c = .2629
r.squaredGLMM(partial2) # R2c = .0360
r.squaredGLMM(partial3) # R2c = .2253
r.squaredGLMM(partial4) # R2c = .2750
r.squaredGLMM(partial5) # R2c = .2371
pseudo_r2 <- c(0.2629, 0.0360, 0.2253, 0.2750, 0.2371);print(pseudo_r2)
  
#printing the best model
summary(partial4)
```


3) Now add _pas_ and its interaction with _task_ to the fixed effects  
    i. how many types of group intercepts (random effects) can you add without ending up with convergence issues or singular fits?  
    ii. create a model by adding random intercepts (without modelling slopes) that results in a singular fit - then use `print(VarCorr(<your.model>), comp='Variance')` to inspect the variance vector - explain why the fit is singular (Hint: read the first paragraph under details in the help for `isSingular`)
    iii. in your own words - how could you explain why your model would result in a singular fit?  
    
#### 3. i-iii. singular fit & convergence

i.When adding different types of group intercepts (random effect), I can only add two intercepts: (1|subject)+(1|trial), before the model meets singular fits issues.


ii.By adding (1|subject)+(1|trial)+(1|task) to the existing model with "pas" and "task" as fixed effects interaction, the model met singular fits issues. When inspecting the variance vector of the model with singular fits (np_model_4), it shows that the reason for the singular fit, is that the model contains a random-effect variance estimate of zero, this random-effect is (1|task). 

iii. I suppose the singular fit is a result of overfitting. The model and its random effects are to complex to be supported by the data. To overcome this, we could remove some random effects one by one.


```{r}
# original model
np_model_1 <- lm(rt.obj ~task +pas + pas*task, data= experiment, REML=FALSE)

#adding random effects until convergence issues and singular fit
np_model_2 <- lmer(rt.obj ~task +pas + pas*task +(1|subject), data= experiment, REML=FALSE)
np_model_3 <- lmer(rt.obj ~task +pas + pas*task +(1|subject)+(1|trial), data= experiment, REML=FALSE)
np_model_4 <- lmer(rt.obj ~task +pas + pas*task +(1|subject)+(1|trial)+ (1|task), data= experiment, REML=FALSE) ### boundary (singular fit)

#cheking out why np_model_4 has a singular fit
print(VarCorr(np_model_4), comp='Variance') # random effect task variance = 0
# when a random effect task variance =  0 (er det fordi den random effect ikke forklarer noget som helst)

```

## Exercise 3

1) Initialize a new data frame, `data.count`. _count_ should indicate the number of times they categorized their experience as _pas_ 1-4 for each _task_. I.e. the data frame would have for subject 1: for task:singles, pas1 was used # times, pas2 was used # times, pas3 was used # times and pas4 was used # times. You would then do the same for task:pairs and task:quadruplet  

#### 1. Data.count dataframe

```{r}
#new dataframe counting number of the 4 pas in each task
data_count <- data %>% 
  group_by(subject, task, pas) %>% 
  summarise("count" = n())

ls.str(data_count)
```        


2) Now fit a multilevel model that models a unique "slope" for _pas_ for each _subject_ with the interaction between _pas_ and _task_ and their main effects being modelled  
    i. which family should be used? 
    ii. why is a slope for _pas_ not really being modelled? 
    iii. if you get a convergence error, try another algorithm (the default is the _Nelder_Mead_) - try (_bobyqa_) for which the `dfoptim` package is needed. In `glmer`, you can add the following for the `control` argument: `glmerControl(optimizer="bobyqa")` (if you are interested, also have a look at the function `allFit`)
    iv. when you have a converging fit - fit a model with only the main effects of _pas_ and _task_. Compare this with the model that also includes the interaction  
    v. indicate which of the two models, you would choose and why  
    vi. based on your chosen model - write a short report on what this says about the distribution of ratings as dependent on _pas_ and _task_  
    vii. include a plot that shows the estimated amount of ratings for four subjects of your choosing
    
#### 2. i-vii. Modelling "count"

i. 
Poisson distribution.

ii.
The "pas" variable is encoded as a factor, meaning that it computes the analysis for each level separately . Thus, we'll get a slope for each separate level of "pas", where each slope is relative to the reference level. 
  

iii.
No convergence error occur if the aforementioned algorithm is used. 
  

iv.
See chunk. 

v.
I would choose countmodel1 as it has the lowest AIC and the highest pseudo R2. I guess, that it conceptually also makes more sense to look at the interaction between "task" and "pas" and not how they individually predict "count". 


vi.
The mixed effect model (count~pas*task + (1+pas|subject)) was constructed to predict "count" from the fixed effect (both main and interaction) between "pas" and "task" and the random slope "pas" given random intercept "subject". The direction of the slope (whether it is positive or negative) is highly dependent on the interaction between pas and task.

Both plots show, how the the rating of "pas" are to a certain extent similar across the 3 tasks. Though, "pas 3" is does not have as many ratings as the other ones.  


vii.

```{r}
#multilevel model 
countmodel1 <- glmer(count~pas*task + (1+pas|subject), data = data_count, family = poisson, control = glmerControl(optimizer="bobyqa")) 

summary(countmodel1)

# Model with only the main effects of pas and task
countmodel2 <- glmer(count ~ pas + task + (1 + pas|subject), data = data_count, family = "poisson", control = glmerControl(optimizer="bobyqa")) 


#akaike information criterion
AIC(countmodel1,countmodel2)

#comparing by R^2 
MuMIn::r.squaredGLMM(countmodel1)
MuMIn::r.squaredGLMM(countmodel2) 

#plot for report
data_count$task <- relevel(data_count$task, ref = "singles")

ggplot(data_count, aes(x = pas, y = count)) +
  geom_point(aes(pas, count), color = "blue") +
  facet_wrap(~ task) +
  theme_bw()
bp <- ggplot(data_count, aes(x=pas, y=count, group=pas)) + 
  geom_boxplot(aes(fill=pas))

bp + facet_grid(task ~ .)

#bp + facet_grid(pas ~ task, margins=TRUE)


# plot for 4 subjects: 001, 002, 003, 004,
data.count_four <- data_count %>% 
  filter(subject == "001" | subject == "002" | subject == "003" | subject == "004")
m_four <- glmer(count ~ pas * task + (pas|subject), data = data.count_four, family = poisson)
data.count_four %>% 
    ggplot() +
      geom_point(aes(x = pas, y = fitted(m_four), color = "Estimated")) + 
      geom_point(aes(x = pas, y = count, color = "Observed")) +
      facet_wrap( ~ subject)

```


3) Finally, fit a multilevel model that models _correct_ as dependent on _task_ with a unique intercept for each _subject_  
    i. does _task_ explain performance?  
    ii. add _pas_ as a main effect on top of _task_ - what are the consequences of that?  
    iii. now fit a multilevel model that models _correct_ as dependent on _pas_ with a unique intercept for each _subject_
    iv. finally, fit a model that models the interaction between _task_ and _pas_  and their main effects  
    v. describe in your words which model is the best in explaining the variance in accuracy  

#### 3. i-v. Modelling "correct"

i.
The output of the model indicates that "task" is significant for all levels but "quaduplet", therefore task does explain performance for "singles" and "pairs". Though, inspecting the coefficients of the model, it makes great sense that "quadruplet" is negative, which indicates more a decrease in "correct" answers, i.e when complexity increases the amount correct answer decreases. Calculated probabilities state: 75% probability of getting 1 (correct) in pair task, 73% probability of getting 1 (correct) in quadruplet task, 78% probability of getting 1 (correct) in single task. Meaning, there is less probability of being correct in the quadruplet task. Though note, these probabilities are still quite similar, so drawing any conclusion from them would probably be faulty. 

ii.
When adding "pas" as a main effect on top of "task", "task" loses its significance and "pas" gains it, indicating "pas" could be a better predictor.
  

iii.
See chunk

iv.
See chunk. I'm not sure whether you mean if we should still keep (1|subject) in the model, but I just do. 

v. 
The model that explains the most variance  is model_correct4 (correct~pas+task+pas*task+ (1|subject)), since it has the lowest R2. Though, model_correct3 has the lowest AIC value, but model_correct4's AIC is almost just as small. In fact model_correct2, model_correct3, and model_correct4 are quite good all of them. 

```{r}
#model with "correct"
model_correct1 <- glmer(correct~task + (1|subject), data= experiment, family = binomial)
#see summary again to see log odds
summary(model_correct1)

#probability getting each task
pair <- invlogit(1.11896);print(pair)
quar <- invlogit(1.11896-0.07496);print(quar)
single <- invlogit(1.118961+0.16603);print(single)

#add _pas_ as a main effect on top of _task_ - what are the consequences of that?  
model_correct2 <- glmer(correct~task +pas + (1|subject), data= experiment, family = binomial)
summary(model_correct2)

#now fit a multilevel model that models _correct_ as dependent on _pas_ with a unique intercept for each _subject_
model_correct3 <- glmer(correct~pas + (1|subject), data= experiment, family=binomial)
summary(model_correct3)


# finally, fit a model that models the interaction between _task_ and _pas_  and their main effects 
model_correct4 <- glmer(correct~pas+task+pas*task+ (1|subject), data= experiment, family=binomial)
summary(model_correct4)


#comparing by AIR and R^2 
MuMIn::r.squaredGLMM(model_correct1)# R2c = 0.10167098
MuMIn::r.squaredGLMM(model_correct2) # R2c = 0.3018983
MuMIn::r.squaredGLMM(model_correct3) # R2c = 0.3015883
MuMIn::r.squaredGLMM(model_correct4) # R2c = 0.3031205 #best

aic<- AIC(model_correct1, model_correct2, model_correct3, model_correct4);print(aic)
```


