---
title: "Assignment 2, Part 1, Methods 3, 2021, autumn semester"
author: "Rikke Uldb√¶k"
date: "September 29th, 2021"
output:
  pdf_document: default
  html_document: default
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/Cognitive Science/3rd semester/Methods 3/github_methods_3/week_03")
pacman::p_load(tidyverse,dplyr, data.table, vroom, ggplot2, readbulk, lme4, rstanarm, MuMIn)
```

# Exercises and objectives
The objectives of the exercises of this assignment are:  
1) Download and organise the data and model and plot staircase responses based on fits of logistic functions  
2) Fit multilevel models for response times  
3) Fit multilevel models for count data  

REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below (__MAKE A KNITTED VERSION__)  
REMEMBER: This assignment will be part of your final portfolio

## Exercise 1

Go to https://osf.io/ecxsj/files/ and download the files associated with Experiment 2 (there should be 29).  
The data is associated with Experiment 2 of the article at the following DOI https://doi.org/10.1016/j.concog.2019.03.007  

1) Put the data from all subjects into a single data frame  
```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#read the data
data <- read_bulk(
  directory= "experiment_2/",
  fun = read_csv
)

attach(data)
```

### 2) Describe the data and construct extra variables from the existing variables  
#### i. add a variable to the data frame and call it _correct_ (have it be a _logical_ variable). Assign a 1 to each row where the subject indicated the correct answer and a 0 to each row where the subject indicated the incorrect answer (__Hint:__ the variable _obj.resp_ indicates whether the subject answered "even", _e_ or "odd", _o_, and the variable _target_type_ indicates what was actually presented.
#### ii. describe what the following variables in the data frame contain, _trial.type_, _pas_, _trial_, _target.contrast_, _cue_, _task_, _target_type_, _rt.subj_, _rt.obj_, _obj.resp_, _subject_ and _correct_. (That means you can ignore the rest of the variables in your description). For each of them, indicate and argue for what `class` they should be classified into, e.g. _factor_, _numeric_ etc.  
  For each of them, indicate and argue for what `class` they should be classified into, e.g. _factor_, _numeric_ etc.  

##### trial.type
This variable contains two different values; "experiment" and "staircase" in which each represent two different trials. When eyeballing this variable, it is evident what we have 69% "experiment" values and 31% "staircase" variables. This variable is currently a character variable and will be classified into a factor for analytic purposes.

```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
class(trial.type)
data$trial.type <- as.factor(data$trial.type)

#plot
n <- nrow(data)  # Number of rows in total
(percent_trial.type<- table(data$trial.type)/n * 100) #generating percentages of trial.type
barplot(percent_trial.type,ylim=c(0,100), ylab="percent",main="Barplot of trial.type")
```

##### pas
The "pas" variable (an abbreviation of Perceptual Awareness Scale), contain 4 different values ranging from 1:4. 1 indicates No Experience, 2 indicates Weak Glimpse (WG), 3 indicates Almost Clear Experience (ACE), and 4 indicates Clear Experience (CE). A histogram of the frequency, shows that the most frequent value is 1 and the least frequent value is 3. This variable is currently a numeric variable and will be classified into a factor for analytic purposes.


```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
class(pas)
summary(pas)
data$pas <- as.factor(data$pas)
hist(pas)
```

##### trial
The "trial" variable contains values ranging from 1:73. This variables indicate that there are 73 different trials in the data set. This variable is currently a numeric variable and will be classified into a factor for analytic purposes.


```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
class(trial)
data$trial <- as.factor(data$trial)
```

##### target.contrast
The "target.contrast" variable is a numeric variable ranging from 0-1, representing the grey-scale proportion of the target digit.
```{r}
class(target.contrast)
ggplot(data, aes(target.contrast))+geom_density()
summary(target.contrast)
```

##### cue
The "cue" variable is a numeric variable, representing a number code for cue.

```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
class(cue)
summary(cue)
hist(cue)
```

##### task
The "task" variable consists of 3 different values: "singles", "pairs" and "quadruplets". These 3 different values each represent the number of digits that would be presented to the participant. An example of "singles" being 2:3, an example of "pairs" being 12:44, and an example of quadruplets being 5432:2345.This variable is currently a character variable and will be classified into a factor for analytic purposes.

```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
class(task)
data$task <- as.factor(data$task)

#plot
n <- nrow(data)  # Number of rows in total
(percent_task<- table(data$task)/n * 100) #generating percentages of trial.type
barplot(percent_task,ylim=c(0,100), ylab="percent",main="Barplot of task")
```

##### target.type
The "target.type" variable represents if the digit (target) was either an even or odd number. 
This variable is currently a character variable and will be classified into a factor for analytic purposes.
```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
class(target.type)
data$target.type <- as.factor(data$target.type)
```

##### rt.subj
The "rt.subj" variable represents the reaction time (seconds) on the PAS response. The variable is numeric and highly right skewed, wwith a minimum value of 0.01369, a maximum value of 32.77161, and a mean of 0.74128.  
```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
class(rt.subj)
ggplot(data, aes(rt.subj))+ geom_density()
summary(rt.subj)
```


##### rt.obj
The variable "rt.obj" represents the reaction time (seconds) on the target digit. This variable is numeric, and highly right skewed, with a minimum value of 0.00004, a maximum value of 291.83119 , and a mean of 1.16186.  
```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
class(rt.obj)
ggplot(data, aes(rt.obj))+ geom_density()
summary(rt.obj)
```


##### obj.resp
The character variable "obj.resp" represents the key actually pressed e for even and o for odd.This variable is currently a character variable and will be classified into a factor for analytic purposes.
```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
class(obj.resp)
data$cue <- as.factor(data$obj.resp) 
```

##### subject
The subject variable represents each participant's individual ID, which makes it easier for us to distinguish between the participants and account for possible individual differences. This variable is a character string containing 29 different subject ID's, and each subject ID has 506 rows (observations). This variable is currently a character variable and will be classified into a factor for analytic purposes.

```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
class(subject)
length(unique(subject)) #amount of different participants
data$subject <- as.factor(data$subject)
```

##### correct
###### Add "correct" variable
The "correct" variable is a logical binary variable containing 1's and 0's. 1 demonstrates when the subject indicated a correct answer (i.e. target_type = "even" and obj.resp = "e", and target_type = "odd" and obj.resp = "o"), and 0 demonstrates when the subject indicated an incorrect answer. 

```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#add a variable and call it "correct"
data$correct <- (ifelse(obj.resp== "o" & target.type == "odd","1",
                               ifelse(obj.resp=="e" & target.type== "even","1", 0)))
#making it as a factor
data$correct <- as.factor(data$correct)
```
 

#### iii. for the staircasing part __only__, create a plot for each subject where you plot the estimated function (on the _target.contrast_ range from 0-1) based on the fitted values of a model (use `glm`) that models _correct_ as dependent on _target.contrast_. These plots will be our _no-pooling_ model. Comment on the fits - do we have enough data to plot the logistic functions? 

I'm not sure whether you mean complete pooling or no pooling, so I only did no pooling.
The fits looks quite poor, because the fitted values does not resemble the sigmoid function. In order to get better fits (looking more sigmoid-like) we might need more data.

```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
## no pooling, glm for each subject 

dfStair = data %>% 
  subset(trial.type == "staircase")
dfStair$subject = gsub("(?<![0-9])0+", "", dfStair$subject, perl = TRUE)
dfStair$subject = as.integer(dfStair$subject)

nopoolfun <- function(i){
  dat <- dfStair[which(dfStair$subject == i),]
  model <- glm(correct~target.contrast, family = 'binomial', data=dat)
  fitted <- model$fitted.values
  plot_dat <- data.frame(cbind(fitted,'target.contrast'=dat$target.contrast))
plot <- ggplot(plot_dat, aes(x = target.contrast, y = fitted))+
  geom_point()+
  geom_line(aes(x = target.contrast, y = fitted))+
  xlab('Target Contrast')+
  ylim(c(0,1))+
  theme_minimal()
return(plot)
}

library(gridExtra)

subjects <- c(1:16)
plots <- lapply(subjects, FUN=nopoolfun)
do.call(grid.arrange,  plots)
subjects <- c(17:29)
plots <- lapply(subjects, FUN=nopoolfun)
do.call(grid.arrange,  plots)

```


#### iv. on top of those plots, add the estimated functions (on the _target.contrast_ range from 0-1) for each subject based on partial pooling model (use `glmer` from the package `lme4`) where unique intercepts and slopes for _target.contrast_ are modelled for each _subject_  


```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#partial pool
partialPoolModel = glmer(correct~target.contrast + (target.contrast|subject), family = 'binomial', dfStair)
dfStair$yEstPartialPooling = fitted(partialPoolModel)
poolNoPoolFun <- function(i){
  dat <- dfStair[which(dfStair$subject == i),]
  model <- glm(correct~target.contrast, family = 'binomial', data=dat)
  fitted <- model$fitted.values
  plot_dat <- data.frame(cbind(fitted,'target.contrast'=dat$target.contrast,'yEstPartialPooling' = dat$yEstPartialPooling))
plot <- ggplot(plot_dat)+
  geom_point(aes(x = target.contrast, y = fitted), color = "green")+
  geom_line(aes(x = target.contrast, y = fitted), color = "green")+
  geom_point(aes(x = target.contrast, y = yEstPartialPooling), color = "red")+
  geom_line(aes(x = target.contrast, y = yEstPartialPooling), color = "red")+
  xlab('Target Contrast')+
  ylim(c(0,1))+
  theme_minimal()
return(plot)
}
subjects <- c(1:16)
plots <- lapply(subjects, FUN=poolNoPoolFun)
do.call(grid.arrange,  plots)
subjects <- c(17:29)
plots <- lapply(subjects, FUN=poolNoPoolFun)
do.call(grid.arrange,  plots)

```
 

#### v. in your own words, describe how the partial pooling model allows for a better fit for each subject. 

In the plot I've compared both partial pooling and no pooling, where the preditcted probabilities are plotted as the following: red is partial pooling and green is no pooling. Compared to no pooling, partial pooling provides better insight about the population (general tendencies), while allowing the model to take individual differences into account (individual baselines), this is visually evident in the plots, where the red graphs (partial pooling) does not vary much in between subject as the green graphs (no pooling). With partial pooling we can both make a great fit for each subject and make it generalizable (in contrast to no pooling). 



## Exercise 2

Now we __only__ look at the _experiment_ trials (_trial.type_)  

### 1) Pick four subjects and plot their Quantile-Quantile (Q-Q) plots for the residuals of their objective response times (_rt.obj_) based on a model where only intercept is modelled  
#### i. comment on these    
#### ii. does a log-transformation of the response time data improve the Q-Q-plots?  

Before log transformation of the 4 subjects objective response time based on the model with only an intercept, the residuals indicate a right skewed tail, as they deviate from the line, i.e deviate from normality. 

After log transformation of the 4 subjects objective response time based on the model with only an intercept, the residuals indicate both a left skewed tail and a tiny right skewed tail. The log transformation of the objective response time data (variable= "rt.obj") improves the Q-Q-plot, as the dots generally falls better on the line , i.e the residuals look more normally distributed.

```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#experiment dataframe
experiment <- data %>% 
  subset(data$trial.type == "experiment")

# df + model for each subject
subject1 <-  experiment %>% 
  filter(subject == "001") %>% 
  mutate("log_rt.obj" = log(rt.obj))

subjects_model1 <- lm(rt.obj~1, data= subject1)
subjects_model_log1 <- lm(log_rt.obj~1, data= subject1) #log trans

# df + model for each subject
subject2 <-  experiment %>% 
  filter(subject == "002") %>% 
  mutate("log_rt.obj" = log(rt.obj))

subjects_model2 <- lm(rt.obj~1, data= subject2)
subjects_model_log2 <- lm(log_rt.obj~1, data= subject2)#log trans

# df + model for each subject
subject3 <-  experiment %>% 
  filter(subject == "003") %>% 
  mutate("log_rt.obj" = log(rt.obj))

subjects_model3 <- lm(rt.obj~1, data= subject3)
subjects_model_log3 <- lm(log_rt.obj~1, data= subject3)#log trans

# df + model for each subject
subject4 <-  experiment %>% 
  filter(subject =="004") %>% 
  mutate("log_rt.obj" = log(rt.obj))

subjects_model4 <- lm(rt.obj~1, data= subject4)
subjects_model_log4 <- lm(log_rt.obj~1, data= subject4)#log trans


#plotting the residuals in a QQ-plot
qqnorm(resid(subjects_model1), col = "blue")
qqline(resid(subjects_model1))

qqnorm(resid(subjects_model2), col = "red")
qqline(resid(subjects_model2))

qqnorm(resid(subjects_model3), col = "green")
qqline(resid(subjects_model3))

qqnorm(resid(subjects_model4), col = "black")
qqline(resid(subjects_model4))


#plotting the residuals after log-transformation
qqnorm(resid(subjects_model_log1), col = "blue")
qqline(resid(subjects_model_log1))

qqnorm(resid(subjects_model_log2), col = "red")
qqline(resid(subjects_model_log2))

qqnorm(resid(subjects_model_log3), col = "green")
qqline(resid(subjects_model_log3))

qqnorm(resid(subjects_model_log4), col = "black")
qqline(resid(subjects_model_log4))
```


### 2) Now do a partial pooling model modelling objective response times as dependent on _task_? (set `REML=FALSE` in your `lmer`-specification)  

#### i. which would you include among your random effects and why? (support your choices with relevant measures, taking into account variance explained and number of parameters going into the modelling)  

First, I built different models, in order to argue which effects to include. The partial pooling model 4 (log(rt.obj) ~ task + (1 + task | subject) + (1 | trial)), has the lowest sigma as well as AIC value. This is the model that predicts objective response times (rt.obj) dependent on task (fixed effects), modelling a random intercepts for subjects and trial as well as random slopes for task. Random intercepts were modeled for subject as one would expect individual baseline performance differences among different participants. Trial as also been modelled as random intercepts as you would expect different trials to vary in difficulty.


#### ii. explain in your own words what your chosen models says about response times between the different tasks  

The estimates for quadruplet and singles are negative, which indicates that participants have a faster objective response time compared to the intercept (pair). When comparing the tasks "quadruplet" and "singles" the people are generally faster to the "single" task. The effect is, however, strongest from changing from "pairs" to "singles" (the respondents generally complete the singles tasks faster, relative to the quadruplet tasks). I logged the rt.obj, due to the result in the previous exercise, where we found the logged version of rt.obj better for use (more normalized residuals).

```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#partial pooling models
partial1 <- lmerTest::lmer(log(rt.obj)~task + (1|subject) + (1|trial), data = experiment, REML=FALSE)

partial2 <- lmerTest::lmer(log(rt.obj)~task + (1|trial), data = experiment, REML=FALSE)

partial3 <- lmerTest::lmer(log(rt.obj)~task + (1|subject), data = experiment, REML=FALSE)

partial4 <- lmerTest::lmer(log(rt.obj) ~ task + (1 + task | subject) + (1 | trial), data = experiment, REML = F)

partial5 <- lmerTest::lmer(log(rt.obj) ~ task + (1 + task | subject), data = experiment, REML=FALSE)

#Akaike information criterion
AIC(partial1, partial2, partial3, partial4, partial5)

#sigma
partial_sigma<- c(sigma(partial1),sigma(partial2),sigma(partial3), sigma(partial4), sigma(partial5));print(partial_sigma)

#pesudo R2
r.squaredGLMM(partial1) # R2c = .2629
r.squaredGLMM(partial2) # R2c = .0360
r.squaredGLMM(partial3) # R2c = .2253
r.squaredGLMM(partial4) # R2c = .2750
r.squaredGLMM(partial5) # R2c = .2371
pseudo_r2 <- c(0.2629, 0.0360, 0.2253, 0.2750, 0.2371);print(pseudo_r2)
  
#printing the best model
summary(partial4)
```


### 3) Now add _pas_ and its interaction with _task_ to the fixed effects  
    i. how many types of group intercepts (random effects) can you add without ending up with convergence issues or singular fits?

When adding different types of group intercepts (random effect), I can only add two intercepts: (1|subject)+(1|trial), before the model meets singular fit issues.
    

#### ii. create a model by adding random intercepts (without modelling slopes) that results in a singular fit - then use `print(VarCorr(<your.model>), comp='Variance')` to inspect the variance vector - explain why the fit is singular (Hint: read the first paragraph under details in the help for `isSingular`)

By adding (1|subject)+(1|trial)+(1|task) to the existing model with "pas" and "task" as fixed effects interaction, the model met singular fits issues. When inspecting the variance vector of the model with singular fits (np_model_4), it shows that the reason for the singular fit, is that the model contains a random-effect variance estimate of zero, this random-effect is (1|task). 


#### iii. in your own words - how could you explain why your model would result in a singular fit?  
I suppose the singular fit is a result of overfitting. The model and its random effects are to complex to be supported by the data. To overcome this, we could remove some random effects one by one.


```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# original model
np_model_1 <- lm(rt.obj ~task +pas + pas*task, data= experiment, REML=FALSE)

#adding random effects until convergence issues and singular fit
np_model_2 <- lmer(rt.obj ~task +pas + pas*task +(1|subject), data= experiment, REML=FALSE)
np_model_3 <- lmer(rt.obj ~task +pas + pas*task +(1|subject)+(1|trial), data= experiment, REML=FALSE)
np_model_4 <- lmer(rt.obj ~task +pas + pas*task +(1|subject)+(1|trial)+ (1|task), data= experiment, REML=FALSE) ### boundary (singular fit)

#cheking out why np_model_4 has a singular fit
print(VarCorr(np_model_4), comp='Variance') # random effect task variance = 0
# when a random effect task variance =  0, this random effect does not explain anything

```

## Exercise 3

### 1) Initialize a new data frame, `data.count`. _count_ should indicate the number of times they categorized their experience as _pas_ 1-4 for each _task_. I.e. the data frame would have for subject 1: for task:singles, pas1 was used # times, pas2 was used # times, pas3 was used # times and pas4 was used # times. You would then do the same for task:pairs and task:quadruplet  

```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#new dataframe counting number of the 4 pas in each task
data_count <- data %>% 
  group_by(subject, task, pas) %>% 
  summarise("count" = n())
```        


### 2) Now fit a multilevel model that models a unique "slope" for _pas_ for each _subject_ with the interaction between _pas_ and _task_ and their main effects being modelled 

```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#multilevel model 
countmodel1 <- glmer(count~pas*task + (1+pas|subject), data = data_count, family = poisson, control = glmerControl(optimizer="bobyqa")) 
```


#### i. which family should be used? 
Poisson distribution because we are dealing with count data.

#### ii. why is a slope for _pas_ not really being modelled? 
The "pas" variable is encoded as a factor, meaning that it computes the analysis for each level separately. Thus, we'll get a slope for each separate level of "pas", where each slope is relative to the reference level.

```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#check why PAS is not really being modelled
summary(countmodel1)
```



#### iii. if you get a convergence error, try another algorithm (the default is the _Nelder_Mead_) - try (_bobyqa_) for which the `dfoptim` package is needed. In `glmer`, you can add the following for the `control` argument: `glmerControl(optimizer="bobyqa")` (if you are interested, also have a look at the function `allFit`)

No convergence error occur if the aforementioned algorithm is used.  

#### iv. when you have a converging fit - fit a model with only the main effects of _pas_ and _task_. Compare this with the model that also includes the interaction 
```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Model with only the main effects of pas and task
countmodel2 <- glmer(count ~ pas + task + (1 + pas|subject), data = data_count, family = "poisson", control = glmerControl(optimizer="bobyqa")) 

#residuals
resids_countmodel1 <- sum(residuals(countmodel1)^2);print(resids_countmodel1)
resids_countmodel2 <- sum(residuals(countmodel2)^2);print(resids_countmodel2)

#akaike information criterion
AIC(countmodel1,countmodel2)

#comparing by R^2 
MuMIn::r.squaredGLMM(countmodel1)
MuMIn::r.squaredGLMM(countmodel2) 
```


#### v. indicate which of the two models, you would choose and why  
I would choose countmodel1 as it has the lowest AIC, lowest residual standard deviation, and the highest pseudo R2. I guess, that it conceptually also makes more sense to look at the interaction between "task" and "pas" and not how they individually predict "count".

#### vi. based on your chosen model - write a short report on what this says about the distribution of ratings as dependent on _pas_ and _task_  
We have used lmerTest (Kuznetsova, Brockhoff and Christensen, 2017) to perform a linear mixed effects analysis of the relationship between 'count', 'task' and 'pas'. Count indicates the number of times the subject categorized their experience as PAS 1-4 for each task. PAS indicates perceptual awareness scale ranging from 1-4, and task being 3 different versions (singles, pairs and quadruplets). The mixed effect model had the following syntax:
 
 count ~ pas * task + ( 1 + pas | subject )
 
The mixed effect model (countmodel1 = count~pas*task + (1+pas|subject)) was constructed to predict "count" from the fixed effect (both main and interaction) between "pas" and "task" and the random slope "pas" given random intercept "subject". The direction of the slope (whether it is positive or negative) is highly dependent on the interaction between pas and task.

Both fixed and random effects accounted for roughly 98% of variance in the pitch variable. The observed interactions between pas and task were significant with  p-values = < 0.05.

The model has a RSS = 699.5, an AIC = 3148.4, and an R2c = 97.5. The model has a lower AIC value than the other model (AIC= 3398.5), and a slightly better R2c and RSS. 

Furthermore, the two plots show, how the the rating of "pas" are to a certain extent similar across the 3 tasks. Though, "pas 3" is does not have as many ratings as the other ones.  


```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#plot for report
data_count$task <- relevel(data_count$task, ref = "singles")

ggplot(data_count, aes(x = pas, y = count)) +
  geom_point(aes(pas, count), color = "blue") +
  facet_wrap(~ task) +
  theme_bw()
```


#### vii. include a plot that shows the estimated amount of ratings for four subjects of your choosing
```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# plot for 4 subjects: 001, 002, 003, 004,
data.count_four <- data_count %>% 
  filter(subject == "001" | subject == "002" | subject == "003" | subject == "004")
m_four <- glmer(count ~ pas * task + (pas|subject), data = data.count_four, family = poisson)
data.count_four %>% 
    ggplot() +
      geom_point(aes(x = pas, y = fitted(m_four), color = "Estimated")) + 
      geom_point(aes(x = pas, y = count, color = "Observed")) +
      facet_wrap( ~ subject)
```


### 3) Finally, fit a multilevel model that models _correct_ as dependent on _task_ with a unique intercept for each _subject_  

#### i. does _task_ explain performance?  
The output of the model indicates that "task" is significant for all levels but "quaduplet", therefore task does explain performance for "singles" and "pairs". Though, inspecting the coefficients of the model, it makes great sense that "quadruplet" is negative, which indicates more a decrease in "correct" answers, i.e when complexity increases the amount correct answer decreases. Calculated probabilities state: 75% probability of getting 1 (correct) in pair task, 73% probability of getting 1 (correct) in quadruplet task, 78% probability of getting 1 (correct) in single task. Meaning, there is less probability of being correct in the quadruplet task. Though note, these probabilities are still quite similar, so drawing any conclusion from them would probably be faulty. 

```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#model with "correct"
model_correct1 <- glmer(correct~task + (1|subject), data= experiment, family = binomial)
#see summary again to see log odds
summary(model_correct1)

#probability getting each task
pair <- invlogit(1.11896);print(pair)
quar <- invlogit(1.11896-0.07496);print(quar)
single <- invlogit(1.118961+0.16603);print(single)
```


#### ii. add _pas_ as a main effect on top of _task_ - what are the consequences of that?  
When adding "pas" as a main effect on top of "task", "task" becomes insignificant and "pas" becomes significant, indicating "pas" could be a better predictor.

```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#add _pas_ as a main effect on top of _task_ - what are the consequences of that?  
model_correct2 <- glmer(correct~task +pas + (1|subject), data= experiment, family = binomial)
summary(model_correct2)
```

#### iii. now fit a multilevel model that models _correct_ as dependent on _pas_ with a unique intercept for each _subject_

```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#now fit a multilevel model that models _correct_ as dependent on _pas_ with a unique intercept for each _subject_
model_correct3 <- glmer(correct~pas + (1|subject), data= experiment, family=binomial)
summary(model_correct3)
```


#### iv. finally, fit a model that models the interaction between _task_ and _pas_  and their main effects  
See chunk. I'm not sure whether you mean if we should still keep (1|subject) in the model, but I just do. 
```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# finally, fit a model that models the interaction between _task_ and _pas_  and their main effects 
model_correct4 <- glmer(correct~pas+task+pas*task+ (1|subject), data= experiment, family=binomial)
summary(model_correct4)
```


#### v. describe in your words which model is the best in explaining the variance in accuracy  
The model that explains the most variance  is model_correct4 (correct~pas+task+pas*task+ (1|subject)), since it has the lowest R2 of 0.30. Though, model_correct3 has the lowest AIC value, but model_correct4's AIC is almost just as small. In fact model_correct2, model_correct3, and model_correct4 are quite good all of them. 
```{r, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#comparing by AIR and R^2 
MuMIn::r.squaredGLMM(model_correct1)# R2c = 0.10167098
MuMIn::r.squaredGLMM(model_correct2) # R2c = 0.3018983
MuMIn::r.squaredGLMM(model_correct3) # R2c = 0.3015883
MuMIn::r.squaredGLMM(model_correct4) # R2c = 0.3031205 #best

aic<- AIC(model_correct1, model_correct2, model_correct3, model_correct4);print(aic)
```

